{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from imutils import face_utils\n",
    "import imutils\n",
    "import dlib\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import models, optimizers\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial emotion recognition\n",
    "\n",
    "## Objectives\n",
    "\n",
    "* Use a NN fully connected based on euclidian distance\n",
    "* Use a CNN based on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where are the following files :\n",
    "# x_train, y_train, images_train, labels_train\n",
    "# haarcascade_frontalface_default.xml\n",
    "# shape_predictor_68_face_landmarks.dat\n",
    "\n",
    "DATA_PATH = \"../bmdata/data/TP_TP4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare class for building the different models\n",
    "\n",
    "## Attributes\n",
    "* path_to_data : where data will be loaded. Here, typicaly DATA_PATH\n",
    "* model_type : \"CNN\" or \"Dense\"\n",
    "* model : the model we'll create and use\n",
    "* X_train, y_train : data to fit the model\n",
    "\n",
    "## Methods\n",
    "* init : initialize the model depending of its type. Load train data.\n",
    "* compile_model : compile the model and show the summary\n",
    "* fit : fit the model and return history\n",
    "* load_weights : load the weights saved on a previous model based on model_type\n",
    "* predict : return the prediction for the given parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionModel():\n",
    "    def __init__(self, path_to_data, model_type = \"Dense\"):\n",
    "        model = models.Sequential()\n",
    "        self.path_to_data = path_to_data\n",
    "\n",
    "        if model_type == \"Dense\":\n",
    "            self.model_type = \"Dense\"\n",
    "            model.add(Dense(512, activation='relu', input_shape=(4624,)))\n",
    "            model.add(Dense(512, activation='relu'))\n",
    "            model.add(Dense(512, activation='relu'))\n",
    "            model.add(Dense(512, activation='relu'))\n",
    "            model.add(Dense(512, activation='relu'))\n",
    "            model.add(Dense(7, activation='softmax'))\n",
    "            \n",
    "            # Preload data to fit\n",
    "            self.X_train = np.load(self.path_to_data + \"/x_train.npy\")\n",
    "            self.y_train = np.load(self.path_to_data + \"/y_train.npy\")\n",
    "\n",
    "        elif model_type == \"CNN\":\n",
    "            self.model_type = \"CNN\"\n",
    "            model.add(Conv2D(16, 3, activation='relu', input_shape=(48,48,1)))\n",
    "            model.add(Conv2D(32, 3, activation='relu'))\n",
    "            model.add(Conv2D(64, 3, activation='relu'))\n",
    "            model.add(MaxPooling2D((2, 2)))\n",
    "            model.add(Conv2D(128, 3, activation='relu'))\n",
    "            model.add(MaxPooling2D((2, 2)))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(64, activation='relu'))\n",
    "            model.add(Dense(7, activation='softmax'))\n",
    "            \n",
    "            # Preload data to fit\n",
    "            self.X_train = np.load(self.path_to_data + \"/images_train.npy\")\n",
    "            print(self.x_train)\n",
    "            self.y_train = np.load(self.path_to_data + \"/labels_train.npy\")\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def compile_model(self, loss, optimizer, metrics):\n",
    "        self.model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "        self.model.summary()\n",
    "    \n",
    "    def fit(self, validation_split=0.2, epochs=200, batch_size=128):\n",
    "        # Shuffle training data\n",
    "        X_train, y_train = shuffle(self.X_train, self.y_train)\n",
    "        \n",
    "        # Fit the model and get the hostory\n",
    "        history = self.model.fit(X_train, y_train, validation_split=validation_split, epochs=epochs, batch_size=batch_size)\n",
    "        \n",
    "        # Save the weights in a .h5 file\n",
    "        self.model.save_weights(self.path_to_data + \"/weights_\" + self.model_type + \".h5\")\n",
    "        return history\n",
    "    \n",
    "    def load_weights(self):\n",
    "        self.model.load_weights(self.path_to_data + \"/weights_\" + self.model_type + \".h5\")\n",
    "    \n",
    "    def predict(self, distances):\n",
    "        prediction = self.model.predict(distances)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose for model type you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose between \"CNN\" or \"Dense\"\n",
    "model_type = \"CNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model, compile and show summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmotionModel(DATA_PATH, model_type)\n",
    "\n",
    "params = {\n",
    "            \"CNN_loss\": \"mean_squared_error\", \"CNN_optimizer\": \"adam\",\n",
    "            \"Dense_loss\": \"categorical_crossentropy\", \"Dense_optimizer\": optimizers.RMSprop(lr=1e-5)\n",
    "         }\n",
    "\n",
    "model.compile_model(loss=params[model_type+\"_loss\"], optimizer=params[model_type+\"_optimizer\"], metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit(validation_split=0.2, epochs=20, batch_size=32)\n",
    "\n",
    "# Plot curves for accuracy and loss after fitting\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly load the weights for the model based on previous learning\n",
    "model.load_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions used\n",
    "\n",
    "## Functions\n",
    "* get_emotion : return emotion in string format from the vector\n",
    "* detect_parts : calculate distances between points and return an array of distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion(emotion):\n",
    "    emotions = ('Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise')\n",
    "    emo = emotions[np.argmax(emotion)]\n",
    "    return emo\n",
    "\n",
    "def detect_parts(image):\n",
    "    # resize the image, and convert it to grayscale\n",
    "    image = imutils.resize(image, width=200, height=200)\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # detect faces in the grayscale image\n",
    "    rects = detector(gray, 1)\n",
    "    \n",
    "    distances = np.zeros((68,68))\n",
    "    \n",
    "    # loop over the face detections\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        distances = np.linalg.norm(shape - shape[:,None], axis=-1)\n",
    "        distances = (distances - np.mean(distances))/ 3*np.std(distances)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live emotion recognizer\n",
    "\n",
    "* Initialize the camera and classifiers\n",
    "* Detect faces\n",
    "* Depending on the model, use images or distances to predict the emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# opencv initialization\n",
    "face_cascade = cv2.CascadeClassifier(DATA_PATH + \"/haarcascade_frontalface_default.xml\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# initialize dlib's face detector and create a predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(DATA_PATH + \"/shape_predictor_68_face_landmarks.dat\")\n",
    "    \n",
    "while (True):\n",
    "    ret, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)  \n",
    "        # draw rectangle to main image\n",
    "        detected_face = img[int(y):int(y + h), int(x):int(x + w)]  \n",
    "        # crop detected face\n",
    "        \n",
    "        if model_type == \"Dense\":\n",
    "            distances = detect_parts(detected_face)\n",
    "            emotion = model.predict(distances.reshape((-1, 4624)))\n",
    "            emotion_text = get_emotion(emotion)\n",
    "        else:\n",
    "            image = gray[int(y):int(y + h), int(x):int(x + w)]\n",
    "            image = np.resize(image, (48,48))\n",
    "            emotion = model.predict((np.reshape(image, (1, 48, 48, 1)))/np.max(image))\n",
    "            emotion_text = get_emotion(emotion)\n",
    "        # write emotion text above rectangle\n",
    "        cv2.putText(img, emotion_text, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.imshow('img', img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  \n",
    "        # press q to quit\n",
    "        break\n",
    "        # kill opencv\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result analysis\n",
    "\n",
    "The two models aren't working very well. After tests, the prediction depends on :\n",
    "* the shape of the face\n",
    "* the light (lamp, sun, clouds)\n",
    "* the normalization (concerning CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
